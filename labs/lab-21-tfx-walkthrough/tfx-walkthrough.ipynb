{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX Components Walk-through\n",
    "\n",
    "The primary goal of this lab is to develop a high level understanding of core TFX components.\n",
    "\n",
    "You will utilize  **TFX Interactive Context** to work with the TFX components interactivelly in a Jupyter notebook environment.\n",
    "\n",
    "Working in an interactive notebook is useful when doing initial data exploration, experimenting with models, and designing ML pipelines. You should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n",
    "\n",
    "In a production deployment of TFX on GCP, you will use an orchestrator such as Kubeflow Pipelines, or Cloud Composer. In an interactive mode, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n",
    "\n",
    "In a production deployment, ML Metadata will be managed in a scalabe database like CloudSQL, and artifacts in apersistent store such as Google Cloud Storage. In an interactive mode, both properties and payloads are stored in the local file system of the Jupyter host.\n",
    "\n",
    "You will work with the [Covertype Data Set](https://github.com/jarokaz/mlops-labs/blob/master/datasets/covertype/README.md) and use TFX  to analyze, understand and pre-process the dataset and train, analyze, validate and deploy the multi-class classification model.\n",
    "\n",
    "\n",
    "The lab is designed to be instructor led. The instructor will walk you through the lab and provide commentary about each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "import tfx\n",
    "\n",
    "from pprint import pprint\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2, statistics_pb2, anomalies_pb2\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import BigQueryExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import ModelValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.components.common_nodes.importer_node import ImporterNode\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.proto.evaluator_pb2 import SingleSlicingSpec\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"TFX Version:\", tfx.__version__)\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure lab settings\n",
    "\n",
    "Set constants, location paths and other environment settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='tfx-covertype-classifier'\n",
    "PIPELINE_ROOT=os.path.join(os.sep, 'home', 'artifact-store', PIPELINE_NAME)\n",
    "os.makedirs(PIPELINE_ROOT, exist_ok=True)\n",
    "\n",
    "SERVING_MODEL_DIR=os.path.join(os.sep, 'home', 'serving_model')\n",
    "os.makedirs(PIPELINE_ROOT, exist_ok=True)\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype/full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Interactive Context\n",
    "\n",
    "TFX Interactive Context allows you to create and run TFX Components in an interactive mode. It is designed to support experimentation and development in a Jupyter Notebook environment. It is an experimental feature and major changes to interface and functionality are expected. When creating the interactive context you can specifiy the following parameters:\n",
    "- `pipeline_name` - Optional name of the pipeline for ML Metadata tracking purposes. If not specified, a name will be generated for you.\n",
    "- `pipeline_root` - Optional path to the root of the pipeline's outputs. If not specified, an ephemeral temporary directory will be created and used.\n",
    "- `metadata_connection_config` - Optional `metadata_store_pb2.ConnectionConfig` instance used to configure connection to a ML Metadata connection. If not specified, an ephemeral SQLite MLMD connection contained in the pipeline_root directory with file name \"metadata.sqlite\" will be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    metadata_connection_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data using ExampleGen\n",
    "\n",
    "In any ML development process the first step  is to ingest the training and test datasets. The `ExampleGen` component ingests data into a TFX pipeline. It consumes external files/services to generate a set file files in the `TFRecord` format,  which will be used by other TFX components. It can also shuffle the data and split into an arbitrary number of partitions.\n",
    "\n",
    "### Configure CsvExampleGen\n",
    "\n",
    "In this exercise, you use the `CsvExampleGen` specialization of `ExampleGen` to ingest CSV files from the GCS location. The component is configured to split the input data into two splits - `train` and `eval` - using 4:1 ratio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=4),\n",
    "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "    ]))\n",
    "\n",
    "example_gen = tfx.components.CsvExampleGen(\n",
    "    instance_name='Data_Extraction_Spliting',\n",
    "    input=external_input(DATA_ROOT),\n",
    "    output_config=output_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the CsvExampleGen component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the ingested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = example_gen.outputs['examples'].get()[0].uri\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "decoder = tfdv.TFExampleDecoder()\n",
    "for tfrecord in dataset.take(2):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = decoder.decode(serialized_example)\n",
    "  pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating statistics using StatisticsGen\n",
    "\n",
    "The `StatisticsGen`  component generates data statistics that can be used by other TFX components. StatisticsGen uses [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started). `StatisticsGen` generates statistics for each split in the `ExampleGen` component's output. In our case there are two splits: `train` and `eval`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and  run the `StatisticsGen` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    instance_name='Statistics_Generation',\n",
    "    examples=example_gen.outputs['examples'])\n",
    "\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize statistics\n",
    "\n",
    "The generated statistics can be visualized using the `tfdv.visualize_statistics()` function from the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library or using a utility method of the `InteractiveContext` object. In fact, most of the artifacts generated by the TFX components can be visualized using `InteractiveContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infering data schema using SchemaGen\n",
    "\n",
    "Some TFX components use a description input data called a schema. The schema is an instance of `schema.proto`. It can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties. `SchemaGen` automatically generates the schema by inferring types, categories, and ranges from data statistics. The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that developers review and modify it as needed. `SchemaGen` uses [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started).\n",
    "\n",
    "The `SchemaGen` component generates the schema using the statistics for the `train` split. The statistics for other splits are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `SchemaGen` components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False)\n",
    "\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the auto-generated schema\n",
    "\n",
    "In most cases the auto-generated schemas must be fine tuned manually using insights from data exploration and/or domain knowledge about the data. For example, you know that in the `covertype` dataset there are seven types of forest cover (coded using 1-7 range) and that the value of the `Slope` feature should be in the 0-90 range. You can manually add these constraints to the auto-generated schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the auto-generated schema proto file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_proto_path = '{}/{}'.format(schema_gen.outputs['schema'].get()[0].uri, 'schema.pbtxt')\n",
    "schema = tfdv.load_schema_text(schema_proto_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the schema\n",
    "\n",
    "You can use the protocol buffer APIs to modify the schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.set_domain(schema, 'Cover_Type', schema_pb2.IntDomain(name='Cover_Type', min=1, max=7, is_categorical=True))\n",
    "tfdv.set_domain(schema, 'Slope',  schema_pb2.IntDomain(name='Slope', min=0, max=90))\n",
    "\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the updated schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dir = '/home/covertype_schema'\n",
    "tf.io.gfile.makedirs(schema_dir)\n",
    "schema_file = os.path.join(schema_dir, 'schema.pbtxt')\n",
    "\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "\n",
    "!cat {schema_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the updated schema using ImporterNode\n",
    "\n",
    "The `ImporterNode` component allows you to import an external artifact, including the schema file, so it can be used by other TFX components in your workflow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `ImporterNode` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_importer = ImporterNode(\n",
    "    instance_name='Schema_Importer',\n",
    "    source_uri=schema_dir,\n",
    "    artifact_type=tfx.types.standard_artifacts.Schema,\n",
    "    reimport=False\n",
    ")\n",
    "\n",
    "context.run(schema_importer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the imported schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_importer.outputs['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating data with ExampleValidator\n",
    "\n",
    "The `ExampleValidator` component identifies anomalies in data.  It identifies anomalies by comparing data statistics computed by the `StatisticsGen` component against a schema generated by `SchemaGen` or imported by `ImporterNode`.\n",
    "\n",
    "`ExampleValidator` can detect different classes of anomalies. For example it can:\n",
    "\n",
    "- perform validity checks by comparing data statistics against a schema \n",
    "- detect training-serving skew by comparing training and serving data.\n",
    "- detect data drift by looking at a series of data.\n",
    "\n",
    "\n",
    "The `ExampleValidator` component validates the data in the `eval` split only. Other splits are ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `ExampleValidator` component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_importer.outputs['result'],\n",
    "    instance_name=\"Data_Validation\"\n",
    ")\n",
    "\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of `ExampleValidator`\n",
    "\n",
    "The output artifact of the ExampleValidator is the `anomalies.pbtxt` file describing an anomalies_pb2.Anomalies protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = example_validator.outputs['anomalies'].get()[0].uri\n",
    "anomalies_filename = os.path.join(train_uri, \"anomalies.pbtxt\")\n",
    "!cat $anomalies_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize validation results\n",
    "\n",
    "The file `anomalies.pbtxt` can be visualized using `context.show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case no anomalies were detected in the `eval` split.\n",
    "\n",
    "For a detailed deep dive into data validation and schema generation refer to the `lab-31-tfdv-structured-data` lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data with Transform\n",
    "\n",
    "The `Transform` component performs data transformation and feature engineering. The `Transform` component consumes `tf.Examples` emitted from the `ExampleGen` component and emits the transformed feature data and the `SavedModel` graph that was used to process the data. The emitted `SavedModel`  can then be used by serving components to make sure that the same data pre-processing logic is applied at training and serving.\n",
    "\n",
    "The `Transform` component requires more code than many other components because of the arbitrary complexity of the feature engineering that you may need for the data and/or model that you're working with. It requires code files to be available which define the processing needed.\n",
    "\n",
    "### Define the pre-processing module\n",
    "\n",
    "To configure `Transform`, you need to encapsulate your pre-processing code in the Python `preprocessing_fn` function and save it to a  python module that is then provided to the Transform component as an input. This module will be loaded by transform and the `preprocessing_fn` function will be called when the `Transform` component runs.\n",
    "\n",
    "In most cases, your implementation of the `preprocessing_fn` makes extensive use of [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) for performing feature engineering on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transform_module = 'covertype_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_transform_module}\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Covertype dataset transformation routines.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "NUMERIC_FEATURES_KEYS = [\n",
    "    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_KEYS = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "LABEL_KEY = 'Cover_Type'\n",
    "\n",
    "\n",
    "def _transformed_name(key):\n",
    "  return key + '_xf'\n",
    "\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replaces missing values and coverts a SparseTensor to a DenseTensor.\"\"\"\n",
    "\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"Preprocesses Covertype Dataset.\"\"\"\n",
    "\n",
    "  outputs = {}\n",
    "\n",
    "  # Scale numerical features\n",
    "  for key in NUMERIC_FEATURES_KEYS:\n",
    "    outputs[_transformed_name(key)] = tft.scale_to_z_score(\n",
    "        _fill_in_missing(inputs[key]))\n",
    "\n",
    "  # Generate vocabularies and maps categorical features\n",
    "  for key in CATEGORICAL_FEATURES_KEYS:\n",
    "    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "        x=_fill_in_missing(inputs[key]), num_oov_buckets=1, vocab_filename=key)\n",
    "\n",
    "  # Convert Cover_Type from 1-7 to 0-6\n",
    "  outputs[_transformed_name(LABEL_KEY)] = _fill_in_missing(\n",
    "      inputs[LABEL_KEY]) - 1\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `Transform` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_importer.outputs['result'],\n",
    "    module_file=_transform_module)\n",
    "\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the `Transform` component's outputs\n",
    "\n",
    "The Transform component has 2 outputs:\n",
    "\n",
    "- `transform_output` - contains the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
    "- `transformed_examples` - contains the preprocessed training and evaluation data.\n",
    "\n",
    "Take a peek at the `transform_output` artifact: it points to a directory containing 3 subdirectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(transform.outputs['transform_output'].get()[0].uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_fn` subdirectory contains the actual preprocessing graph. The `metadata` subdirectory contains the schema of the original data. The `transformed_metadata` subdirectory contains the schema of the preprocessed data.\n",
    "\n",
    "The `transformed_examples` folder contains `TFRecord` files with transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_uri = transform.outputs['transformed_examples'].get()[1].uri\n",
    "tfrecord_filenames = [os.path.join(transform_uri, name)\n",
    "                      for name in os.listdir(transform_uri)]\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "decoder = tfdv.TFExampleDecoder()\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = decoder.decode(serialized_example)\n",
    "  pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the `Trainer` component\n",
    "\n",
    "The `Trainer` component trains a model using TensorFlow.\n",
    "\n",
    "`Trainer` takes:\n",
    "\n",
    "- tf.Examples used for training and eval.\n",
    "- A user provided module file that defines the trainer logic.\n",
    "- A data schema created by `SchemaGen` or imported by `ImporterNode`.\n",
    "- A proto definition of train args and eval args.\n",
    "- An optional transform graph produced by upstream Transform component.\n",
    "- An optional base models used for scenarios such as warmstart.\n",
    "\n",
    "Trainer generates  a `SavedModel` and an `EvalSavedModel`. \n",
    "\n",
    "#### Define the trainer module\n",
    "\n",
    "To configure `Trainer`, you need to encapsulate your training code in a Python module that is then provided to the `Trainer` as an input. The module must include the `trainer_fn` function that must return an `tf.estimator` based estimator. If you prefer to work with `Keras`, you can do so and then convert the Keras model to an estimator using the `tf.keras.model_to_estimator()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'covertype_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Covertype Classifier training function.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "LABEL_KEY = 'Cover_Type'\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "EXPORTED_MODEL_NAME = 'covertype-classifier'\n",
    "\n",
    "\n",
    "def _transformed_name(key):\n",
    "  return key + '_xf'\n",
    "\n",
    "\n",
    "def _get_raw_feature_spec(schema):\n",
    "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Returns a TFRecord reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _build_estimator(config,\n",
    "                     numeric_feature_keys,\n",
    "                     categorical_feature_keys,\n",
    "                     hidden_units,\n",
    "                     warm_start_from=None):\n",
    "  \"\"\"Build an estimator for predicting forest cover based on cartographic data.\"\"\"\n",
    "\n",
    "  num_feature_columns = [\n",
    "      tf.feature_column.numeric_column(key) for key in numeric_feature_keys\n",
    "  ]\n",
    "  categorical_feature_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=num_buckets, default_value=0)\n",
    "      for key, num_buckets in categorical_feature_keys\n",
    "  ]\n",
    "\n",
    "  return tf.estimator.DNNLinearCombinedClassifier(\n",
    "      config=config,\n",
    "      n_classes=NUM_CLASSES,\n",
    "      linear_feature_columns=categorical_feature_columns,\n",
    "      dnn_feature_columns=num_feature_columns,\n",
    "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
    "      warm_start_from=warm_start_from)\n",
    "\n",
    "\n",
    "def _input_fn(filenames, feature_specs, label_key, batch_size=200):\n",
    "  \"\"\"Generates features and labels for training or evaluation.\"\"\"\n",
    "\n",
    "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=filenames,\n",
    "      batch_size=batch_size,\n",
    "      features=feature_specs,\n",
    "      label_key=label_key,\n",
    "      reader=_gzip_reader_fn)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def _example_serving_receiver_fn(tf_transform_output, schema, label_key):\n",
    "  \"\"\"Builds the serving graph.\"\"\"\n",
    "\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "  raw_feature_spec.pop(label_key)\n",
    "\n",
    "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "      raw_feature_spec, default_batch_size=None)\n",
    "  serving_input_receiver = raw_input_fn()\n",
    "\n",
    "  transformed_features = tf_transform_output.transform_raw_features(\n",
    "      serving_input_receiver.features)\n",
    "\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      transformed_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "\n",
    "def _eval_input_receiver_fn(tf_transform_output, schema, label_key):\n",
    "  \"\"\"Builds everything needed for the tf-model-analysis to run the model.\"\"\"\n",
    "\n",
    "  # Notice that the inputs are raw features, not transformed features here.\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "\n",
    "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "      raw_feature_spec, default_batch_size=None)\n",
    "  serving_input_receiver = raw_input_fn()\n",
    "\n",
    "  features = serving_input_receiver.features.copy()\n",
    "  transformed_features = tf_transform_output.transform_raw_features(features)\n",
    "\n",
    "  # NOTE: Model is driven by transformed features (since training works on the\n",
    "  # materialized output of TFT, but slicing will happen on raw features.\n",
    "  features.update(transformed_features)\n",
    "\n",
    "  return tfma.export.EvalInputReceiver(\n",
    "      features=features,\n",
    "      receiver_tensors=serving_input_receiver.receiver_tensors,\n",
    "      labels=transformed_features[label_key])\n",
    "\n",
    "\n",
    "def trainer_fn(hparams, schema):\n",
    "  \"\"\"Builds the objects required by TFX Transform.\"\"\"\n",
    "\n",
    "  train_batch_size = 40\n",
    "  eval_batch_size = 40\n",
    "  hidden_units = [128, 64]\n",
    "\n",
    "  # Retrieve transformed feature specs\n",
    "  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n",
    "  transformed_feature_spec = (\n",
    "      tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "  print(transformed_feature_spec)\n",
    "  print(type(transformed_feature_spec))\n",
    "\n",
    "  # Prepare transformed feature name lists\n",
    "  # For categorical features retrieve vocabulary sizes\n",
    "  transformed_label_key = _transformed_name(LABEL_KEY)\n",
    "  transformed_numeric_feature_keys = [\n",
    "      _transformed_name(key) for key in NUMERIC_FEATURE_KEYS\n",
    "  ]\n",
    "  transformed_categorical_feature_keys = [\n",
    "      (_transformed_name(key),\n",
    "       tf_transform_output.num_buckets_for_transformed_feature(\n",
    "           _transformed_name(key))) for key in CATEGORICAL_FEATURE_KEYS\n",
    "  ]\n",
    "\n",
    "  # Create a training input function\n",
    "  train_input_fn = lambda: _input_fn(\n",
    "      filenames=hparams.train_files,\n",
    "      feature_specs=tf_transform_output.transformed_feature_spec().copy(),\n",
    "      batch_size=train_batch_size,\n",
    "      label_key=transformed_label_key)\n",
    "\n",
    "  # Create an evaluation input function\n",
    "  eval_input_fn = lambda: _input_fn(\n",
    "      filenames=hparams.eval_files,\n",
    "      feature_specs=tf_transform_output.transformed_feature_spec().copy(),\n",
    "      batch_size=eval_batch_size,\n",
    "      label_key=transformed_label_key)\n",
    "\n",
    "  # Create a training specification\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "      train_input_fn, max_steps=hparams.train_steps)\n",
    "\n",
    "  # Create an evaluation specifaction\n",
    "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
    "      tf_transform_output, schema, LABEL_KEY)\n",
    "  exporter = tf.estimator.FinalExporter(EXPORTED_MODEL_NAME,\n",
    "                                        serving_receiver_fn)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "      eval_input_fn,\n",
    "      steps=hparams.eval_steps,\n",
    "      exporters=[exporter],\n",
    "      name=EXPORTED_MODEL_NAME)\n",
    "\n",
    "  # Create runtime config\n",
    "  run_config = tf.estimator.RunConfig(\n",
    "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
    "\n",
    "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
    "\n",
    "  # Build an estimator\n",
    "  estimator = _build_estimator(\n",
    "      hidden_units=hidden_units,\n",
    "      numeric_feature_keys=transformed_numeric_feature_keys,\n",
    "      categorical_feature_keys=transformed_categorical_feature_keys,\n",
    "      config=run_config,\n",
    "      warm_start_from=hparams.warm_start_from)\n",
    "\n",
    "  # Create an input receiver for TFMA processing\n",
    "  receiver_fn = lambda: _eval_input_receiver_fn(tf_transform_output, schema,\n",
    "                                                transformed_label_key)\n",
    "\n",
    "  return {\n",
    "      'estimator': estimator,\n",
    "      'train_spec': train_spec,\n",
    "      'eval_spec': eval_spec,\n",
    "      'eval_input_receiver_fn': receiver_fn\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and run the Trainer component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    module_file=_trainer_module_file,\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    schema=schema_importer.outputs['result'],\n",
    "    transform_output=transform.outputs['transform_output'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing training runs with TensorBoard\n",
    "\n",
    "In this step you will analyze the training run with [TensorBoard.dev](https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html). `TensorBoard.dev` is a managed service that enables you to easily host, track and share your ML experiments.\n",
    "\n",
    "*There are some issues with the support for TensorBoard in the current release of AI Platform Notebooks. This is the reason for using TensorBoard.dev. When the issues are addressed the lab will be updated to use the built-in support for TensorBoard.*\n",
    "\n",
    "### Retrieve the location of TensorBoard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = trainer.outputs['model'].get()[0].uri\n",
    "logs_path = os.path.join(train_uri, 'serving_model_dir')\n",
    "print(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the logs and start TensorBoard.dev\n",
    "\n",
    "1. Open a new JupyterLab terminal window\n",
    "\n",
    "2. From the terminal window, execute the following command\n",
    "```\n",
    "tensorboard dev upload --logdir [YOUR_LOGDIR]\n",
    "```\n",
    "\n",
    "Where [YOUR_LOGDIR] is an URI retrieved by the previous cell.\n",
    "\n",
    "You will be asked to authorize `TensorBoard.dev` using your Google account. If you don't have a Google account or you don't want to authorize `TensorBoard.dev` you can skip this exercise.\n",
    "\n",
    "After the authorization process completes, follow the link provided to view your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating trained models with Evaluator\n",
    "The `Evaluator` component analyzes model performance using the [TensorFlow Model Analysis library](https://www.tensorflow.org/tfx/model_analysis/get_started). It runs inference requests on particular subsets of the test dataset, based on which slices are defined by the developer. Knowing which slices should be analyzed requires domain knowledge of what is important in this particular use case or domain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the Evaluator component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    ")\n",
    "context.run(model_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize evaluation results\n",
    "You can visualize the evaluation results using the `tfma.view.render_slicing_metrics()` function from TensorFlow Model Analysis library.\n",
    "\n",
    "*Currently, there is an issue in the JupyterLab on AI Platform Notebooks that prevents `tfma.view.render_slicing_metrics()` from rendering. We will keep monitoring the issue and update this part of the lab as required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_uri = model_analyzer.outputs['output'].get()[0].uri\n",
    "#eval_result = tfma.load_eval_result(evaluation_uri)\n",
    "#tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model with the ModelValidator Component\n",
    "\n",
    "The `ModelValidator` Component helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.\n",
    "\n",
    "`ModelValidator` compares new models against a baseline (such as the currently serving model) to determine if they're \"good enough\" relative to the baseline. It does so by evaluating both models on an eval dataset and computing their performance on metrics (e.g. AUC, loss). If the new model's metrics meet developer-specified criteria relative to the baseline model (e.g. AUC is not lower), the model is \"blessed\" (marked as good), indicating to the Pusher that it is ok to push the model to production.\n",
    "\n",
    "Note: Currently developers can only specify criteria metrics for the whole evaluation split (dataset). A future version will support more granular criteria such as slices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `ModelValidator` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator = ModelValidator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'])\n",
    "context.run(model_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of `ModelValidator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blessing_uri = model_validator.outputs.blessing.get()[0].uri\n",
    "!ls -l {blessing_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models with Pusher\n",
    "\n",
    "The `Pusher` component checks whether a model has been \"blessed\", and if so, deploys it by pushing the model to a well known file destination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the `Pusher` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_serving_model_dir = os.path.join(os.sep, 'home', 'serving_model', 'covertype_classifier')\n",
    "\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=model_validator.outputs['blessing'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of `Pusher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_pushed_model = os.path.join(_serving_model_dir, max(os.listdir(_serving_model_dir)))\n",
    "!saved_model_cli show --dir {latest_pushed_model} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This concludes the lab `lab-31-tfx-components-walkthrough`. The next labs in the series will guide through developing a TFX pipeline, deploying and running the pipeline on **Kubeflow Pipelines** and automating the pipeline build and deployment processes with **Cloud Build**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
